# LSTM å¤šæ­¥é æ¸¬æ¨¡å‹ - ä½¿ç”¨æŒ‡å—

## å¿«é€Ÿé–‹å§‹

### 1. å®‰è£ä¾è³´

```bash
pip install torch numpy pandas scikit-learn pyyaml
```

### 2. æº–å‚™è³‡æ–™

ç¢ºä¿ä½ çš„ CSV æª”æ¡ˆåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š

```
timestamp, open, high, low, close, volume
2024-01-01, 45000, 45500, 44800, 45200, 1500000
2024-01-02, 45200, 45800, 45000, 45500, 1600000
...
```

### 3. ä¿®æ”¹é…ç½®

ç·¨è¼¯ `config.yaml`ï¼š

```yaml
data_path: 'your_data.csv'  # æ”¹æˆä½ çš„æ•¸æ“šè·¯å¾‘
lookback: 100               # éå» 100 æ ¹ K ç·š
forecast_horizon: 6         # é æ¸¬æœªä¾† 6 æ ¹ K ç·š (å·²å„ªåŒ–)
```

### 4. é‹è¡Œè¨“ç·´

```bash
python test/run_pipeline_pytorch.py
```

### 5. æŸ¥çœ‹çµæœ

```
=== æœªä¾† 6 æ ¹ K ç·šé æ¸¬ ===
Kç·š 1: Open=45150.23, High=45450.67, Low=45000.12, Close=45300.45, Volume=1520000
Kç·š 2: Open=45300.45, High=45600.89, Low=45200.34, Close=45450.56, Volume=1550000
...
```

## æœ€æ–°æ¨¡å‹: Attention-LSTM

### æ¶æ§‹ç‰¹å¾µ

```
è¼¸å…¥åºåˆ— (100æ­¥, 17ç‰¹å¾µ)
    â†“
Encoder LSTM (2å±¤, 256éš±å±¤)
    â†“
Multi-Head Attention (4é ­) â† æ–°å¢ï¼
    â†“
Decoder LSTM (2å±¤, 256éš±å±¤)
    â†“
è¼¸å‡ºåºåˆ— (6æ­¥, 17ç‰¹å¾µ)
```

### æ€§èƒ½æŒ‡æ¨™ (å·²é©—è­‰)

| æŒ‡æ¨™ | æ•¸å€¼ | è©•ç´š |
|------|------|------|
| RMSE | 0.853 | â­â­â­â­ |
| RÂ² Score | 0.452 | â­â­â­â­ |
| Std Ratio | 0.721 | â­â­â­â­â­ |
| MAE | 0.670 | â­â­â­â­ |

**æ”¹é€² vs åŸºç·š**:
- RMSE: 1.096 â†’ 0.853 (22% â†“)
- RÂ² Score: 0.097 â†’ 0.452 (365% â†‘)
- Std Ratio: 0.297 â†’ 0.721 (142% â†‘)

### ç‚ºä»€éº¼ç”¨ Attention-LSTMï¼Ÿ

**å‚³çµ± Encoder-Decoder çš„å•é¡Œ**:
- ç„¡æ³•å€åˆ†é‡è¦å’Œä¸é‡è¦çš„æ™‚é–“æ­¥
- æ‰€æœ‰æ­·å²æ•¸æ“šæ¬Šé‡ç›¸åŒ
- é æ¸¬æ¨™æº–å·®åä½ (0.30)

**Attention æ©Ÿåˆ¶çš„è§£æ±º**:
- Multi-Head Attention å­¸ç¿’å“ªäº›æ™‚é–“æ­¥é‡è¦
- è‡ªå‹•åŠ æ¬Šæ­·å²ç‰¹å¾µ
- æ¨™æº–å·®æ¯”ä¾‹æå‡åˆ° 0.72

### è¨“ç·´éç¨‹

```
Epoch 10:  Val Loss = 0.1794
Epoch 20:  Val Loss = 0.1814 (å¹³å°æœŸ)
Epoch 30:  Val Loss = 0.1816 
Epoch 40:  Val Loss = 0.1727 â†“ (é–‹å§‹æ”¹å–„)
Epoch 50:  Val Loss = 0.1678 â†“
Epoch 70:  Val Loss = 0.1585 â†“
Epoch 80:  Val Loss = 0.1438 â†“â†“ (æ˜é¡¯æ”¹å–„)
Epoch 100: Val Loss = 0.1297 âœ“ (æœ€å„ª)
```

**è¨“ç·´æ™‚é–“**: ~150 åˆ†é˜ (GPU RTX 4060 Ti)

## æª”æ¡ˆèªªæ˜

### `run_pipeline_pytorch.py` (ä¸»ç¨‹å¼)

**é—œéµé¡åˆ¥**:

1. **`MultiHeadAttention` é¡** (æ–°)
   - 4 å€‹æ³¨æ„åŠ›é ­
   - ä¸¦è¡Œè¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
   - è‡ªå‹•å­¸ç¿’ç‰¹å¾µé‡è¦æ€§

2. **`AttentionLSTM` æ¨¡å‹** (æ–°)
   - Encoder LSTM: å£“ç¸®è¼¸å…¥åºåˆ—
   - Attention: åŠ æ¬Šé‡è¦ç‰¹å¾µ
   - Decoder LSTM: ç”Ÿæˆé æ¸¬åºåˆ—
   - LayerNorm: è¨“ç·´ç©©å®šæ€§

3. **`calculate_technical_indicators(df)`** (å·²æœ‰)
   - è¨ˆç®— 17 å€‹æŠ€è¡“æŒ‡æ¨™
   - åŒ…å«æ³¢å‹•ç‡ã€ATRã€RSIã€MACDã€KAMA ç­‰
   - è‡ªå‹•å¡«å…… NaN å€¼

4. **`train_model()` å‡½æ•¸** (å·²æœ‰)
   - ä½¿ç”¨ Adam å„ªåŒ–å™¨ (AMSGrad)
   - å­¸ç¿’ç‡è‡ªå‹•èª¿æ•´ (ReduceLROnPlateau)
   - Early stopping: 50 å€‹ epoch ç„¡æ”¹é€²å‰‡åœæ­¢
   - æ¢¯åº¦è£å‰ª: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

### `config.yaml` (é…ç½®æ–‡ä»¶)

```yaml
# æ ¸å¿ƒåƒæ•¸
lookback: 100            # è¼¸å…¥æ™‚é–“æ­¥ (åŸ 60 â†’ 100)
forecast_horizon: 6      # è¼¸å‡ºæ™‚é–“æ­¥ (åŸ 15 â†’ 6)
hidden_size: 256         # LSTM éš±å±¤ç¶­åº¦
num_layers: 2            # LSTM å±¤æ•¸

# è¨“ç·´åƒæ•¸
batch_size: 32
learning_rate: 0.001
epochs: 100
patience: 50             # Early stopping è€å¿ƒå€¼ (åŸ 20 â†’ 50)

# ç‰¹å¾µåˆ—è¡¨
features: [returns, log_returns, volatility_20, volatility_5, atr, rsi, ...]
```

### `models/attention_lstm.pt` (å·²è¨“ç·´æ¨¡å‹)

é è¨“ç·´çš„ Attention-LSTM æ¬Šé‡ï¼Œå¯ç›´æ¥ç”¨æ–¼æ¨ç†ã€‚

## æ¨¡å‹æ¶æ§‹è©³è§£

### Encoder-Decoder ç‚ºä»€éº¼æœ‰æ•ˆï¼Ÿ

**å•é¡Œ**: ç°¡å–® LSTM é€²è¡Œå¤šæ­¥é æ¸¬æœ‰èª¤å·®ç´¯ç©
- é æ¸¬ç¬¬ 1 æ­¥ â†’ è¼¸å…¥ç¬¬ 2 æ­¥
- ç¬¬ 1 æ­¥èª¤å·® â†’ ç¬¬ 2 æ­¥è¼¸å…¥èª¤å·® â†’ èª¤å·®éè¿´æ”¾å¤§
- 15 æ­¥å¾Œèª¤å·®æ¥µå¤§

**è§£æ±º**: Encoder-Decoder å¹³è¡Œé æ¸¬æ‰€æœ‰æ­¥
- Encoder: è®€å–éå» 100 æ ¹ K ç·š
- Decoder: ç›´æ¥é æ¸¬æœªä¾† 6 æ ¹ K ç·š
- æ¯æ­¥ç¨ç«‹ç”Ÿæˆï¼Œç„¡èª¤å·®ç´¯ç©

### Attention å¦‚ä½•æ”¹é€²ï¼Ÿ

**å‚³çµ± Encoder-Decoder çš„ç¼ºé™·**:
```python
# æ‰€æœ‰æ™‚é–“æ­¥æ¬Šé‡ç›¸åŒ
hidden_state = encoder_output[-1]  # åªç”¨æœ€å¾Œä¸€æ­¥
```

**Attention æ”¹é€²**:
```python
# è‡ªå‹•å­¸ç¿’æ¯æ­¥çš„é‡è¦æ€§
attention_weights = softmax(query @ key.T / sqrt(d))
context = attention_weights @ value  # åŠ æ¬Šçµ„åˆ
```

**çµæœ**:
- é‡è¦æ™‚é–“æ­¥ç²å¾—æ›´é«˜æ¬Šé‡
- æ¨¡å‹å­¸ç¿’é•·æœŸä¾è³´
- é æ¸¬æ³¢å‹•æ€§æ›´æº–ç¢º

### ç‰¹å¾µå·¥ç¨‹æ ¸å¿ƒ

**17 å€‹ç‰¹å¾µçš„çµ„æˆ**:

| é¡åˆ¥ | ç‰¹å¾µ | ä½œç”¨ |
|------|------|------|
| **åŸºç¤** | returns, log_returns | æ•¸æ“šæ­£è¦åŒ– |
| **æ³¢å‹•ç‡** | volatility_20, volatility_5, atr | æ•æ‰é¢¨éšªè®ŠåŒ– |
| **å‹•é‡** | rsi, macd, macd_signal, macd_diff | è¶…è²·/è¶…è³£/è½‰æŠ˜é» |
| **å‡ç·š** | sma_7, sma_14, sma_21, kama | å¤šæ™‚é–“é€±æœŸè¶¨å‹¢ |
| **æ¯”ç‡** | high_low_ratio, open_close_ratio | å…§éƒ¨åƒ¹æ ¼çµæ§‹ |
| **ç›¸å°** | price_to_sma_10, price_to_sma_20 | åƒ¹æ ¼ç›¸å°ä½ç½® |

**æ¨™æº–åŒ–å¾ˆé—œéµ**:
```python
StandardScaler()  # Z-score normalization
# æ‰€æœ‰ç‰¹å¾µç¸®æ”¾åˆ°å‡å€¼ 0, æ¨™æº–å·® 1
# ç„¡è«–åŸå§‹ç¯„åœå¤šå¤§ (volume: ç™¾è¬, price: åƒ)
# çµæœéƒ½æ˜¯å¯æ¯”è¼ƒçš„
```

## å¸¸è¦‹å•é¡Œ

### Q1: RÂ² = 0.452 ä»£è¡¨ä»€éº¼ï¼Ÿ

**ç­”**: æ¨¡å‹è§£é‡‹äº† 45.2% çš„åƒ¹æ ¼æ³¢å‹•

- Baseline (é æ¸¬å¹³å‡å€¼): RÂ² = 0
- ç°¡å–® LSTM: RÂ² â‰ˆ 0.10
- **ç¾åœ¨çš„æ¨¡å‹: RÂ² = 0.452** âœ“
- é«˜æ€§èƒ½æ¨¡å‹: RÂ² > 0.7

**å¯¦éš›å«ç¾©**:
- é æ¸¬ä¸æœƒéƒ½é›†ä¸­åœ¨å¹³å‡å€¼
- æ¨¡å‹æ•æ‰åˆ°äº†å¸‚å ´å‹•æ…‹
- å¯ç”¨æ–¼äº¤æ˜“ä¿¡è™Ÿè¼”åŠ©
- ä¸èƒ½å–®ç¨ä¾è³´ (éœ€è¦é¢¨éšªç®¡ç†)

### Q2: RMSE = 0.853 åœ¨å¯¦éš›äº¤æ˜“ä¸­æœ‰å¤šå¤§ç”¨è™•ï¼Ÿ

**ç­”**: å¯è½‰æ›ç‚ºåƒ¹æ ¼èª¤å·®

```
BTC 15åˆ†é˜æ¨™æº–å·® â‰ˆ 500-1000 USD
RMSE = 0.853 Ã— 750 USD â‰ˆ 640 USD

å³: é æ¸¬èª¤å·® Â±640 USD
```

**è©•ä¼°**:
- âœ“ å°äº¤æ˜“æ–¹å‘æœ‰åƒè€ƒåƒ¹å€¼
- âœ“ å¯ç”¨æ–¼é¢¨éšªè©•ä¼°
- âš ï¸ ä¸é©åˆç²¾ç¢ºåƒ¹æ ¼é æ¸¬
- âš ï¸ éœ€èˆ‡æ­¢æçµåˆä½¿ç”¨

### Q3: Std Ratio = 0.721 æ„å‘³è‘—ä»€éº¼æ”¹é€²ï¼Ÿ

**ç­”**: æ¨¡å‹ç¾åœ¨é æ¸¬æ³¢å‹•æ€§æ¥è¿‘çœŸå¯¦

```
ä¹‹å‰: 0.297 (é æ¸¬æ³¢å‹• = å¯¦éš›æ³¢å‹•çš„ 30%) âœ— åš´é‡ä½ä¼°
ç¾åœ¨: 0.721 (é æ¸¬æ³¢å‹• = å¯¦éš›æ³¢å‹•çš„ 72%) âœ“ æ¥è¿‘çœŸå¯¦
å®Œç¾: 1.000 (é æ¸¬æ³¢å‹• = å¯¦éš›æ³¢å‹•)      â† ç›®æ¨™
```

**æ”¹å–„æ„ç¾©**:
- æ¨¡å‹ä¸å†å‚¾å‘é æ¸¬å¹³å‡å€¼
- é«˜æ³¢å‹•å¸‚å ´è¢«æ­£ç¢ºè­˜åˆ¥
- é¢¨éšªç‰¹å¾µè¢«å­¸ç¿’

### Q4: æ€æ¨£é€²ä¸€æ­¥æ”¹é€²æ¨¡å‹ï¼Ÿ

**å¿«é€Ÿæ”¹é€²** (1-2å°æ™‚):
1. å¢åŠ è¨“ç·´ epochs (100 â†’ 150)
   - é æœŸ: RÂ² â†’ 0.48-0.52

2. å¢åŠ  Attention é ­æ•¸ (4 â†’ 8)
   - é æœŸ: RÂ² â†’ 0.50-0.55

3. å¢åŠ éš±å±¤å¤§å° (256 â†’ 384)
   - é æœŸ: RÂ² â†’ 0.48-0.53

**é€²éšæ”¹é€²** (3-4å°æ™‚):
1. å˜—è©¦ Transformer æ¶æ§‹
   - é æœŸ: RÂ² â†’ 0.50-0.60
   - è¨“ç·´æ™‚é–“ç¿»å€

2. ç‰¹å¾µå·¥ç¨‹å„ªåŒ–
   - ç§»é™¤ä½ç›¸é—œç‰¹å¾µ
   - å¢åŠ åƒ¹æ ¼å‹•é‡æŒ‡æ¨™

3. æå¤±å‡½æ•¸èª¿æ•´
   - æ”¹ç”¨ Weighted MSE
   - å°ç•°å¸¸æ³¢å‹•çµ¦äºˆæ›´å¤§æ¬Šé‡

**æœ€å¤§åŒ–æ€§èƒ½** (1-2å¤©):
1. é›†æˆå¤šå€‹æ¨¡å‹
   - é æœŸ: RÂ² â†’ 0.55-0.65

2. å¤šä»»å‹™å­¸ç¿’
   - åŒæ™‚é æ¸¬åƒ¹æ ¼ + æ³¢å‹•ç‡

3. å¼·åŒ–å­¸ç¿’å¾®èª¿
   - åŸºæ–¼äº¤æ˜“å›å ±å„ªåŒ–

### Q5: æ€æ¨£ç”¨é æ¸¬çµæœäº¤æ˜“ï¼Ÿ

**ç°¡å–®ç­–ç•¥**:

```python
# ç²å–ä¸‹ä¸€æ ¹ K ç·šé æ¸¬
open_price = 45150.23
high_price = 45450.67
close_price = 45300.45

# è¨ˆç®—é æœŸè®ŠåŒ–
expected_change = (close_price - open_price) / open_price * 100

if expected_change > 1.0:      # é æœŸçœ‹æ¼²
    signal = "BUY"
    take_profit = open_price * 1.02
    stop_loss = open_price * 0.99
elif expected_change < -1.0:   # é æœŸçœ‹è·Œ
    signal = "SELL"
    take_profit = open_price * 0.98
    stop_loss = open_price * 1.01
else:                          # ç„¡æ˜ç¢ºä¿¡è™Ÿ
    signal = "HOLD"
```

**é‡è¦æé†’**:
- æ¨¡å‹åªæ˜¯è¼”åŠ©å·¥å…·ï¼Œä¸æ‡‰å®Œå…¨ä¾è³´
- **é¢¨éšªç®¡ç†æ¯”æº–ç¢ºç‡æ›´é‡è¦**
- å§‹çµ‚è¨­å®šæ­¢æå’Œæ­¢ç›ˆ
- å°é¡æ¸¬è©¦å¾Œå†æ“´å¤§è¦æ¨¡

## è©•ä¼°æ¨¡å‹æ•ˆèƒ½

### è‡ªå‹•è¨ˆç®—çš„æŒ‡æ¨™

**RMSE (Root Mean Squared Error)** - ç•¶å‰å€¼: 0.853
```
è¶Šå°è¶Šå¥½
< 0.6:   å„ªç§€
0.6-0.9: è‰¯å¥½ âœ“ (æˆ‘å€‘åœ¨é€™è£¡)
> 0.9:   éœ€è¦æ”¹é€²
```

**MAE (Mean Absolute Error)** - ç•¶å‰å€¼: 0.670
```
å¹³å‡çµ•å°èª¤å·®
< 0.5:   å„ªç§€
0.5-0.8: è‰¯å¥½ âœ“ (æˆ‘å€‘åœ¨é€™è£¡)
> 0.8:   éœ€è¦æ”¹é€²
```

**RÂ² Score** - ç•¶å‰å€¼: 0.452
```
è§£é‡‹æ–¹å·®æ¯”ä¾‹
> 0.5:   å¾ˆå¥½
0.3-0.5: å¯ç”¨ âœ“ (æˆ‘å€‘åœ¨é€™è£¡)
< 0.3:   éœ€è¦æ”¹é€²
```

**Std Ratio** - ç•¶å‰å€¼: 0.721
```
é æ¸¬æ³¢å‹• / å¯¦éš›æ³¢å‹•
0.7-1.0: å„ªç§€ âœ“ (æˆ‘å€‘åœ¨é€™è£¡)
0.3-0.7: éœ€è¦æ”¹é€²
< 0.3:   åš´é‡å•é¡Œ
```

### æª¢æŸ¥éæ“¬åˆ

```python
# åœ¨è¨“ç·´æ—¥èªŒä¸­æŸ¥çœ‹æå¤±
Train Loss: 0.0850  (è¨“ç·´æå¤±)
Val Loss:   0.1297  (é©—è­‰æå¤±)

# å¥åº·æŒ‡æ¨™
# Train Loss ç•¥ä½æ–¼ Val Loss æ˜¯æ­£å¸¸çš„
# æ¯”å€¼ (Train/Val) æ‡‰åœ¨ 0.6-0.8 ä¹‹é–“
ratio = 0.0850 / 0.1297 = 0.655 âœ“ å¥åº·
```

## ä¸‹ä¸€æ­¥

1. âœ… è¨“ç·´ Attention-LSTM å®Œæˆ
2. âœ… é©—è­‰æ¨¡å‹æ€§èƒ½é”æ¨™
3. ğŸ“ ç·¨å¯«æ¨ç†è…³æœ¬
4. ğŸ§ª å°é¡å¯¦ç›¤æ¸¬è©¦
5. ğŸ“Š æ”¶é›†çœŸå¯¦äº¤æ˜“æ•¸æ“š
6. ğŸ”„ æŒçºŒæ”¹é€²å’Œå¾®èª¿

---

**éœ€è¦å¹«åŠ©ï¼Ÿ** æŸ¥çœ‹ `feature-engineering-guide.md` ç²å¾—æ›´å¤šæŠ€è¡“ç´°ç¯€ã€‚

**æœ€æ–°æ›´æ–°**: 2026-01-15
- âœ¨ åŠ å…¥ Attention æ©Ÿåˆ¶
- ğŸ¯ é æ¸¬æ­¥æ•¸å„ªåŒ–åˆ° 6 æ­¥
- ğŸ“ˆ æ€§èƒ½æŒ‡æ¨™é”åˆ°å¯¦ç”¨ç·š
